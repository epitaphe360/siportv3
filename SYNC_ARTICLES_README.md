# Synchronisation Automatique des Articles

## Vue d'ensemble

Le syst√®me de synchronisation automatique permet de r√©cup√©rer les articles depuis le site officiel SIPORTS (https://siportevent.com/actualite-portuaire/) et de les afficher dans l'application.

## Architecture

### 1. Edge Function Supabase : `sync-news-articles`

**Emplacement** : `supabase/functions/sync-news-articles/index.ts`

**Fonctionnalit√©s** :
- Scrappe les articles depuis https://siportevent.com/actualite-portuaire/
- Extrait les informations : titre, extrait, image, cat√©gorie, lien source
- Ins√®re les nouveaux articles dans la base de donn√©es
- Met √† jour les articles existants (d√©tection par titre)
- Retourne les statistiques de synchronisation

**D√©ploiement** :
```bash
supabase functions deploy sync-news-articles
```

### 2. Store Frontend : `newsStore.ts`

**Emplacement** : `src/store/newsStore.ts`

**Fonction modifi√©e** : `fetchFromOfficialSite()`

Cette fonction appelle l'Edge Function `sync-news-articles` et recharge ensuite les articles depuis la base de donn√©es.

### 3. Interface Utilisateur : `NewsPage.tsx`

**Emplacement** : `src/pages/NewsPage.tsx`

**Bouton "Actualiser"** :
- D√©clenche la synchronisation manuellement
- Affiche un toast de progression
- Affiche les statistiques de synchronisation (nouveaux articles, mis √† jour)

## Utilisation

### Synchronisation Manuelle

1. Acc√©der √† la page des actualit√©s : `/news`
2. Cliquer sur le bouton **"Actualiser"** (ic√¥ne RefreshCw)
3. Attendre la fin de la synchronisation
4. Les nouveaux articles apparaissent automatiquement

### Synchronisation Automatique (Optionnel)

Pour configurer une synchronisation automatique p√©riodique :

#### Option 1 : Cron Job Supabase

1. Acc√©der au dashboard Supabase
2. Aller dans "Database" > "Cron Jobs"
3. Cr√©er un nouveau cron job :
   ```sql
   SELECT cron.schedule(
     'sync-news-articles-daily',
     '0 6 * * *',  -- Tous les jours √† 6h du matin
     $$
     SELECT net.http_post(
       url := 'https://your-project.supabase.co/functions/v1/sync-news-articles',
       headers := '{"Content-Type": "application/json", "Authorization": "Bearer YOUR_SERVICE_ROLE_KEY"}'::jsonb,
       body := '{}'::jsonb
     );
     $$
   );
   ```

#### Option 2 : GitHub Actions

Cr√©er un fichier `.github/workflows/sync-articles.yml` :

```yaml
name: Sync Articles

on:
  schedule:
    - cron: '0 6 * * *'  # Tous les jours √† 6h UTC
  workflow_dispatch:  # Permet de d√©clencher manuellement

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Call Supabase Edge Function
        run: |
          curl -X POST \
            https://your-project.supabase.co/functions/v1/sync-news-articles \
            -H "Authorization: Bearer ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" \
            -H "Content-Type: application/json" \
            -d '{}'
```

## Structure des Donn√©es

### Table `news_articles`

```sql
CREATE TABLE news_articles (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  title TEXT NOT NULL,
  excerpt TEXT,
  content TEXT NOT NULL,
  category TEXT,
  image_url TEXT,
  tags TEXT[],
  published BOOLEAN DEFAULT true,
  published_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  views INTEGER DEFAULT 0,
  author_id UUID,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

## Scraping

### S√©lecteurs Utilis√©s

Le scraper utilise les s√©lecteurs suivants pour extraire les articles du site officiel :

- **Articles** : `article.elementor-post, .elementor-post`
- **Titre** : `.elementor-post__title a, h2 a, h3 a`
- **Image** : `img[src], img[data-src]`
- **Extrait** : `.elementor-post__excerpt, .elementor-post__text, p`
- **Cat√©gorie** : `.elementor-post__badge, .category`

### Gestion des Erreurs

- Si un article ne peut pas √™tre pars√©, il est ignor√© et un warning est logg√©
- Si aucun article n'est trouv√©, la synchronisation retourne un succ√®s avec 0 articles
- Les erreurs de base de donn√©es sont captur√©es et retourn√©es dans la r√©ponse

## Monitoring

### Logs de l'Edge Function

Pour voir les logs de synchronisation :

1. Acc√©der au dashboard Supabase
2. Aller dans "Edge Functions"
3. S√©lectionner `sync-news-articles`
4. Consulter les logs en temps r√©el

### Logs Typiques

```
üîç Scraping articles from: https://siportevent.com/actualite-portuaire/
‚úÖ Fetched HTML (95539 characters)
üìù Found 6 article elements
‚úÖ Article 1: Casablanca: Development of its port complex...
‚úÖ Article 2: Port Glossary: Understanding Maritime...
üì¶ Syncing 6 articles to database...
‚ú® Inserted: Casablanca: Development of its port complex...
üîÑ Updated: Port Glossary: Understanding Maritime...
üìä Sync Summary:
   ‚ú® Inserted: 4
   üîÑ Updated: 2
   ‚ùå Errors: 0
   üìù Total: 6
```

## D√©pannage

### Probl√®me : Aucun article n'est trouv√©

**Solution** :
1. V√©rifier que le site https://siportevent.com/actualite-portuaire/ est accessible
2. V√©rifier que les s√©lecteurs CSS sont toujours valides
3. Consulter les logs de l'Edge Function

### Probl√®me : Erreur de base de donn√©es

**Solution** :
1. V√©rifier que la table `news_articles` existe
2. V√©rifier les permissions RLS (Row Level Security)
3. V√©rifier que la cl√© `SUPABASE_SERVICE_ROLE_KEY` est correcte

### Probl√®me : Les articles ne s'affichent pas

**Solution** :
1. V√©rifier que les articles ont `published = true`
2. V√©rifier que `published_at` n'est pas dans le futur
3. Recharger la page des actualit√©s

## Am√©liorations Futures

### 1. Scraping du Contenu Complet

Actuellement, seul l'extrait est r√©cup√©r√©. Pour r√©cup√©rer le contenu complet :

```typescript
async function fetchFullArticle(url: string): Promise<string> {
  const response = await fetch(url);
  const html = await response.text();
  const document = new DOMParser().parseFromString(html, 'text/html');
  const contentElement = document.querySelector('.entry-content, .post-content');
  return contentElement?.textContent?.trim() || '';
}
```

### 2. D√©tection des Doublons par URL

Am√©liorer la d√©tection des doublons en utilisant l'URL source :

```typescript
const { data: existing } = await supabaseClient
  .from('news_articles')
  .select('id')
  .or(`title.eq.${article.title},source_url.eq.${article.sourceUrl}`)
  .maybeSingle();
```

### 3. Pagination

G√©rer la pagination pour r√©cup√©rer tous les articles :

```typescript
for (let page = 1; page <= 3; page++) {
  const articles = await scrapeArticles(`${NEWS_URL}page/${page}/`);
  await syncArticlesToDatabase(supabaseClient, articles);
}
```

### 4. Webhooks

Configurer un webhook pour √™tre notifi√© lors de nouveaux articles sur le site officiel.

## Conclusion

Le syst√®me de synchronisation automatique permet de maintenir les articles √† jour sans intervention manuelle. Il peut √™tre d√©clench√© manuellement via l'interface ou automatiquement via un cron job.
